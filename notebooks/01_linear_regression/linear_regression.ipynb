{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "## Concept Summary\n",
    "\n",
    "Linear regression models the relationship between a dependent variable $y$ and one or more independent variables $X$ by fitting a linear equation:\n",
    "\n",
    "$$\\hat{y} = w \\cdot X + b$$\n",
    "\n",
    "where:\n",
    "- $w$ (weights) — how much each feature contributes to the prediction\n",
    "- $b$ (bias) — the baseline prediction when all features are zero\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Formula | Purpose |\n",
    "|---------|---------|--------|\n",
    "| **Hypothesis** | $f_{w,b}(x) = wx + b$ | Predict output given input |\n",
    "| **Cost Function (MSE)** | $J(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m}(f_{w,b}(x^{(i)}) - y^{(i)})^2$ | Measure how wrong the model is |\n",
    "| **Gradient Descent** | $w = w - \\alpha \\frac{\\partial J}{\\partial w}$ | Iteratively minimize the cost |\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- **Level 1**: Implement linear regression from scratch using NumPy\n",
    "- **Level 2**: Use scikit-learn to achieve the same result\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 1: Linear Regression from Scratch\n",
    "\n",
    "In this section, you will implement:\n",
    "1. The cost function\n",
    "2. The gradient computation\n",
    "3. Gradient descent\n",
    "4. Prediction\n",
    "\n",
    "Use only **NumPy** — no scikit-learn allowed here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Generate Synthetic Data\n",
    "\n",
    "We start with a simple dataset so you can verify your implementation easily.\n",
    "\n",
    "The true relationship is: $y = 3x + 7 + \\text{noise}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "m = 100  # number of training examples\n",
    "X_train = 2 * np.random.rand(m)\n",
    "y_train = 3 * X_train + 7 + np.random.randn(m) * 0.5\n",
    "\n",
    "# Visualize\n",
    "plt.scatter(X_train, y_train, alpha=0.6)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Training Data\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Implement the Cost Function\n",
    "\n",
    "The Mean Squared Error cost function:\n",
    "\n",
    "$$J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "where $f_{w,b}(x) = wx + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute the MSE cost for linear regression.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Input features, shape (m,)\n",
    "        y (np.ndarray): Target values, shape (m,)\n",
    "        w (float): Weight parameter\n",
    "        b (float): Bias parameter\n",
    "\n",
    "    Returns:\n",
    "        float: The MSE cost\n",
    "    \"\"\"\n",
    "    m = len(X)\n",
    "\n",
    "    # TODO: Compute the cost\n",
    "    # Step 1: Compute predictions (f_wb) for all training examples\n",
    "    # Step 2: Compute the squared errors\n",
    "    # Step 3: Return the mean (divided by 2m)\n",
    "\n",
    "    cost = 0  # Replace this\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Validation: Cost Function ---\n",
    "# With w=3, b=7 (close to the true values), cost should be small (~0.12)\n",
    "# With w=0, b=0 (bad guess), cost should be large (~35)\n",
    "\n",
    "cost_good = compute_cost(X_train, y_train, w=3, b=7)\n",
    "cost_bad = compute_cost(X_train, y_train, w=0, b=0)\n",
    "\n",
    "print(f\"Cost with w=3, b=7 (good guess): {cost_good:.4f}  (expected: ~0.12)\")\n",
    "print(f\"Cost with w=0, b=0 (bad guess):  {cost_bad:.4f}  (expected: ~35)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Implement the Gradient\n",
    "\n",
    "Compute the partial derivatives of $J$ with respect to $w$ and $b$:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)}) \\cdot x^{(i)}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute the gradients of the cost function w.r.t. w and b.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Input features, shape (m,)\n",
    "        y (np.ndarray): Target values, shape (m,)\n",
    "        w (float): Weight parameter\n",
    "        b (float): Bias parameter\n",
    "\n",
    "    Returns:\n",
    "        tuple: (dj_dw, dj_db) — partial derivatives of J w.r.t. w and b\n",
    "    \"\"\"\n",
    "    m = len(X)\n",
    "\n",
    "    # TODO: Compute the gradients\n",
    "    # Step 1: Compute predictions (f_wb)\n",
    "    # Step 2: Compute the error (prediction - actual)\n",
    "    # Step 3: Compute dj_dw (mean of error * X)\n",
    "    # Step 4: Compute dj_db (mean of error)\n",
    "\n",
    "    dj_dw = 0  # Replace this\n",
    "    dj_db = 0  # Replace this\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Validation: Gradient ---\n",
    "# At w=0, b=0, gradients should be negative (need to increase w and b)\n",
    "\n",
    "dj_dw, dj_db = compute_gradient(X_train, y_train, w=0, b=0)\n",
    "print(f\"dj_dw at (w=0, b=0): {dj_dw:.4f}  (expected: negative, around -11)\")\n",
    "print(f\"dj_db at (w=0, b=0): {dj_db:.4f}  (expected: negative, around -8)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Implement Gradient Descent\n",
    "\n",
    "Repeatedly update $w$ and $b$ using the gradients:\n",
    "\n",
    "$$w = w - \\alpha \\frac{\\partial J}{\\partial w}$$\n",
    "$$b = b - \\alpha \\frac{\\partial J}{\\partial b}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_init, b_init, learning_rate, num_iterations):\n",
    "    \"\"\"\n",
    "    Run gradient descent to learn w and b.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Input features, shape (m,)\n",
    "        y (np.ndarray): Target values, shape (m,)\n",
    "        w_init (float): Initial weight\n",
    "        b_init (float): Initial bias\n",
    "        learning_rate (float): Step size (alpha)\n",
    "        num_iterations (int): Number of gradient descent steps\n",
    "\n",
    "    Returns:\n",
    "        tuple: (w, b, cost_history)\n",
    "            - w: learned weight\n",
    "            - b: learned bias\n",
    "            - cost_history: list of cost values at each iteration\n",
    "    \"\"\"\n",
    "    w = w_init\n",
    "    b = b_init\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # TODO: Implement one step of gradient descent\n",
    "        # Step 1: Compute the gradients using your compute_gradient function\n",
    "        # Step 2: Update w and b simultaneously\n",
    "        # Step 3: Record the cost for plotting\n",
    "\n",
    "        pass  # Replace this\n",
    "\n",
    "        # Print progress every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            cost = compute_cost(X, y, w, b)\n",
    "            cost_history.append(cost)\n",
    "            print(f\"Iteration {i:4d}: Cost = {cost:.6f}, w = {w:.4f}, b = {b:.4f}\")\n",
    "\n",
    "    return w, b, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run Gradient Descent ---\n",
    "w_final, b_final, cost_history = gradient_descent(\n",
    "    X_train, y_train,\n",
    "    w_init=0,\n",
    "    b_init=0,\n",
    "    learning_rate=0.1,\n",
    "    num_iterations=1000\n",
    ")\n",
    "\n",
    "print(f\"\\nLearned parameters: w = {w_final:.4f}, b = {b_final:.4f}\")\n",
    "print(f\"Expected (approx):  w = 3.0000, b = 7.0000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize Results ---\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Data and fitted line\n",
    "axes[0].scatter(X_train, y_train, alpha=0.6, label=\"Training data\")\n",
    "x_line = np.linspace(0, 2, 100)\n",
    "axes[0].plot(x_line, w_final * x_line + b_final, color=\"red\", linewidth=2, label=f\"Fit: y = {w_final:.2f}x + {b_final:.2f}\")\n",
    "axes[0].set_xlabel(\"X\")\n",
    "axes[0].set_ylabel(\"y\")\n",
    "axes[0].set_title(\"Linear Regression Fit\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Cost over iterations\n",
    "axes[1].plot(range(0, 1000, 100), cost_history)\n",
    "axes[1].set_xlabel(\"Iteration\")\n",
    "axes[1].set_ylabel(\"Cost\")\n",
    "axes[1].set_title(\"Cost Function Convergence\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Level 2: Linear Regression with scikit-learn\n",
    "\n",
    "Now re-implement the same task using scikit-learn. Compare the results with your Level 1 implementation.\n",
    "\n",
    "**Key scikit-learn classes:**\n",
    "- `LinearRegression` — fits a linear model using Ordinary Least Squares\n",
    "- The model exposes `.coef_` (weights) and `.intercept_` (bias) after fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn expects X to be 2D: shape (m, n_features)\n",
    "X_train_2d = X_train.reshape(-1, 1)\n",
    "\n",
    "# TODO: Create a LinearRegression model, fit it, and extract the parameters\n",
    "# Step 1: Create the model\n",
    "# Step 2: Fit the model on X_train_2d and y_train\n",
    "# Step 3: Get the learned weight (model.coef_[0]) and bias (model.intercept_)\n",
    "\n",
    "sklearn_w = 0  # Replace this\n",
    "sklearn_b = 0  # Replace this\n",
    "\n",
    "print(f\"scikit-learn: w = {sklearn_w:.4f}, b = {sklearn_b:.4f}\")\n",
    "print(f\"Your Level 1: w = {w_final:.4f}, b = {b_final:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make predictions using the sklearn model and evaluate\n",
    "# Step 1: Use model.predict() on X_train_2d\n",
    "# Step 2: Compute MSE using mean_squared_error(y_train, predictions)\n",
    "# Step 3: Compute R² using r2_score(y_train, predictions)\n",
    "\n",
    "y_pred = np.zeros_like(y_train)  # Replace this\n",
    "\n",
    "mse = 0  # Replace this\n",
    "r2 = 0   # Replace this\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize: Compare Level 1 vs Level 2 ---\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X_train, y_train, alpha=0.5, label=\"Training data\")\n",
    "\n",
    "x_line = np.linspace(0, 2, 100)\n",
    "plt.plot(x_line, w_final * x_line + b_final, color=\"red\", linewidth=2, label=f\"Level 1 (scratch): y={w_final:.2f}x+{b_final:.2f}\")\n",
    "plt.plot(x_line, sklearn_w * x_line + sklearn_b, color=\"green\", linewidth=2, linestyle=\"--\", label=f\"Level 2 (sklearn): y={sklearn_w:.2f}x+{sklearn_b:.2f}\")\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Level 1 vs Level 2 Comparison\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "\n",
    "After completing Level 1 and Level 2, answer these in a markdown cell below:\n",
    "\n",
    "1. How close are your Level 1 parameters to the scikit-learn results? Why might they differ?\n",
    "2. What happens if you change the learning rate to 0.01? To 1.0?\n",
    "3. What happens if you increase the number of iterations to 10000?\n",
    "4. scikit-learn uses **Ordinary Least Squares** (closed-form solution), not gradient descent. What are the trade-offs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answers here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Level 3: Real-World Application\n",
    "\n",
    "For Level 3, you will work in the `src/linear_regression/` module to build a proper Python project that:\n",
    "- Loads a real dataset (California Housing from scikit-learn)\n",
    "- Preprocesses and splits the data\n",
    "- Trains a linear regression model\n",
    "- Evaluates performance with proper metrics\n",
    "\n",
    "Go to `src/linear_regression/` and follow the TODOs in:\n",
    "- `model.py` — your model wrapper class\n",
    "- `pipeline.py` — data loading, training, and evaluation\n",
    "\n",
    "Run tests with: `pytest tests/test_linear_regression.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
