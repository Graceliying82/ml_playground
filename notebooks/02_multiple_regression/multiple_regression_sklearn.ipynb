{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Regression with Multiple Features — Level 2: scikit-learn\n",
    "\n",
    "In this notebook, you will re-implement multiple feature regression using **scikit-learn** and compare with your Level 1 (from-scratch) implementation.\n",
    "\n",
    "### Key scikit-learn Classes\n",
    "\n",
    "| Class/Function | Purpose |\n",
    "|---------------|--------|\n",
    "| `LinearRegression` | Fits a linear model (handles multiple features natively) |\n",
    "| `StandardScaler` | Standardizes features (z-score normalization) |\n",
    "| `model.fit(X, y)` | Train the model |\n",
    "| `model.predict(X)` | Make predictions |\n",
    "| `model.coef_` | Learned weight vector (one per feature) |\n",
    "| `model.intercept_` | Learned bias |\n",
    "| `mean_squared_error` | Compute MSE |\n",
    "| `r2_score` | Compute R² (1.0 = perfect fit) |\n",
    "\n",
    "### What's New in Level 2\n",
    "\n",
    "- `StandardScaler` replaces your manual `zscore_normalize` function\n",
    "- `LinearRegression` handles any number of features\n",
    "- `model.coef_` is now a vector (one weight per feature)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 2.1 Generate the Same Synthetic Data\n",
    "\n",
    "We use the same multi-feature data as Level 1 so we can directly compare results.\n",
    "\n",
    "True relationship: $y = 0.5 x_1 + 2.0 x_2 + 0.1 x_3 + 4 + \\text{noise}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000\n",
    "n = 3\n",
    "\n",
    "X_train = np.column_stack([\n",
    "    500 + 3000 * np.random.rand(m),\n",
    "    1 + 4 * np.random.rand(m),\n",
    "    100 * np.random.rand(m),\n",
    "])\n",
    "\n",
    "w_true = np.array([0.5, 2.0, 0.1])\n",
    "b_true = 4.0\n",
    "\n",
    "y_train = X_train @ w_true + b_true + np.random.randn(m) * 5\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2.2 Feature Scaling with StandardScaler\n",
    "\n",
    "In Level 1, you implemented z-score normalization manually. scikit-learn provides `StandardScaler` to do this.\n",
    "\n",
    "**Important**: Fit the scaler on training data only, then use `transform()` on both train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scale features using StandardScaler\n",
    "# Step 1: Create a StandardScaler instance\n",
    "# Step 2: Fit and transform X_train using fit_transform()\n",
    "# Step 3: Print the mean and std before/after scaling\n",
    "\n",
    "raise NotImplementedError(\"Implement feature scaling with StandardScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2.3 Fit the Model\n",
    "\n",
    "Create a `LinearRegression` model and fit it on the scaled data.\n",
    "\n",
    "Note: scikit-learn's `LinearRegression` uses the **Normal Equation** (closed-form), not gradient descent. It doesn't strictly require feature scaling, but scaling still helps with numerical stability and interpreting coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and fit a LinearRegression model\n",
    "# Step 1: Create a LinearRegression instance\n",
    "# Step 2: Fit on X_train_scaled and y_train\n",
    "# Step 3: Print the learned weights (model.coef_) and bias (model.intercept_)\n",
    "\n",
    "raise NotImplementedError(\"Implement model fitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 2.4 Make Predictions and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make predictions and compute metrics\n",
    "# Step 1: Predict using model.predict(X_train_scaled)\n",
    "# Step 2: Compute MSE using mean_squared_error()\n",
    "# Step 3: Compute R² using r2_score()\n",
    "# Step 4: Print the metrics\n",
    "\n",
    "raise NotImplementedError(\"Implement predictions and evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 2.5 Fit Without Scaling (Compare)\n",
    "\n",
    "scikit-learn's `LinearRegression` can fit without scaling. Let's see how the coefficients compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit LinearRegression on raw (unscaled) X_train\n",
    "# Step 1: Create a new LinearRegression and fit on X_train (raw)\n",
    "# Step 2: Print the learned weights — they should be close to w_true!\n",
    "# Step 3: Compare with the scaled version's weights\n",
    "\n",
    "raise NotImplementedError(\"Implement unscaled model for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 2.6 Visualize: Feature Weights Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a bar chart comparing:\n",
    "# 1. True weights (w_true)\n",
    "# 2. sklearn weights (unscaled model — should match true weights)\n",
    "# 3. Your Level 1 weights (copy from your Level 1 results)\n",
    "#\n",
    "# Hint: Use plt.bar() with different x positions for each group\n",
    "\n",
    "feature_names = [\"Size\", \"Bedrooms\", \"Age\"]\n",
    "\n",
    "# Your visualization code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "\n",
    "Answer these in the markdown cell below:\n",
    "\n",
    "1. When you fit on raw data, `model.coef_` matches the true weights. When you fit on scaled data, it doesn't. Why?\n",
    "2. If sklearn doesn't require scaling, why is scaling still useful in practice?\n",
    "3. sklearn uses the Normal Equation (closed-form), not gradient descent. What are the trade-offs?\n",
    "4. How do the coefficients from the scaled model help you understand feature importance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "*Your answers here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Level 2 complete! Continue to:\n",
    "- **Level 3**: `multiple_regression_real_world.ipynb` — Apply to the Diabetes dataset with production-style code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
